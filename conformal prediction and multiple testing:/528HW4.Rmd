---
title: "528HW4"
author: "Coco_Luo"
date: "2023-02-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

Let X be a 50-dimensional random vector that is distributed according to $N(0,I)$, and $y = 3 tanh \beta^T X + \epsilon$, we sample coordinates of $\beta$ from a normal distribition $N(0, 1)$ and fix this for the entire analysis.

### 1.1 

For 100 independent trials, I generated the pair $(X_{n+1},y_{n+1})$. Then, I use the training data $\{(X_i,Y_i)\}$ and the test data $X_{n+1}$, and apply the split conformal prediction procedure (with a linear pre- diction model) to obtain a prediction interval for $y_{n+1}$. The empirical probabilities of $y_{n+1}$ belong in this prediction interval over the 100 independent trials are shown below ($\alpha = 0.05$). 

```{r, echo=FALSE}
md = matrix(c(0.95, 0.94, 0.96, 0.97,
       -224.02, -144.99, -4.73, -4.09,
       196.18, 235.77, 5.07, 3.89) ,nrow=3, ncol=4, byrow=TRUE)
colnames(md) = c(50, 100, 200, 400)
rownames(md) = c('empirical probrobability', 'mean upper bound',' mean lower bound')
knitr::kable(md, caption = "linear model simulation")
```

The table shows us that, with linear regression stimulation, the probability that true value in the interval is greater when we have greater sample sizes, and the interval gets smaller when we have smaller sample sizes.

```{r, include=FALSE}
set.seed(2023)
beta <- rnorm(50)
alpha = 0.05

library(MASS)
library(randomForest)
library(mvtnorm)

sim <- function(n){
  size = n + 1
  X = MASS::mvnorm(n=n,  mu = rep(0, 50), Sigma = diag(1, 50))
  error = rnorm(size)
  y = 3 * tanh(X %*% beta) + error
  
  model <- lm(y ~ ., data = df_sim[1:n_size/2, ])
  resid <- sort(abs(df_sim[n_size/2 + 1:n_size, ]$y - predict(model, newdata = df_sim[n_size/2 + 1:n_size, 1:50])))
  interval <- quantile(resid, 1 - alpha)
  mu_predict <- predict(model, newdata = df_sim[size, 1:50])
  conformal_interval <- c(mu_predict - interval, mu_predict + interval)
  return(list(In_interval = (df_sim[size, ]$y >= conformal_interval[1] & df_sim[size, ]$y <= conformal_interval[2]), range = conformal_interval))
}
```

### 1.2

```{r, echo=FALSE}
md2 = matrix(c(0.93, 0.92, 0.91, 0.96,
       -4.85, -4.65, -4.57, -4.20,
       4.55, 4.56, 4.33, 4.46) ,nrow=3, ncol=4, byrow=TRUE)
colnames(md2) = c(50, 100, 200, 400)
rownames(md2) = c('empirical probrobability', 'mean upper bound',' mean lower bound')
knitr::kable(md2, caption = "Random forest simulation")
```

The table shows us that, with random forest model stimulation, the empirical probability of true value is higher than the previous linear regression model. For the linear model, the conformal intervals in linear model are crazy because of overfitting (we are using half of the total sample to fit the model). However, random forest does not have the same problem and it generates more stable intervals.


```{r, include=FALSE}
sim2 <- function(n) {
  size <- n + 1
  X<- mvrnorm(n = size, mu = rep(0,3))
  error <- rnorm(size)
  y <- 3 * tanh(X %*% beta) + error
  df_sim <- data.frame(x = X, y = y)
  model <- randomForest(y ~ ., data = df_sim[1:n_size/2, ])
  resid <- sort(abs(df_sim[n_size/2 + 1:n_size, ]$y - predict(model, newdata = df_sim[n_size/2 + 1:n_size, 1:50])))
  
  
  interval <- quantile(resid, 1 - alpha)
  mu_predict <- predict(model, newdata = df_sim[size, 1:50]) 
  conformal_interval<-c(mu_predict - interval, mu_predict +interval)
  
  return(list(In_interval = (df_sim[size, ]$y >= conformal_interval[1] & df_sim[size, ]$y <= conformal_interval[2]), range = conformal_interval))
}
```


## Question 2

In this question I will analyze data described in Efron and Hastie (2016). In a prostate cancer study, data were collected on 50 controls and 52 patients with cancer, with the genetic activity being measured at m = 6033 genes. The investigators were hoping to spot non-null genes, ones for which patients and controls would respond differently. The gene.txt file contains 6033 x-scores. Under the null, these statistics follow an N(0, 1) distribution. 

### 2.1

First, I calculate the p-values corresponding to the z-scores and generate a histogram of these values. 
\newline

```{r, echo=FALSE}
dat <- read.table("~/Desktop/528/HW4/gene.txt", quote="\"", comment.char="")
colnames(dat) <- c('z-score')
```

```{r, echo=FALSE}
dat$p_values <- 2 * pnorm(q = -abs(dat$`z-score`))
hist(dat$p_values, breaks = 30, col = "grey", main = "Two-Tailed P-Values", xlab = "P-Value", ylab = "Frequency")
```

### 2.2

Next, let's see which genes are flagged as significant under 4 different procedures. For the first one

a). Bonferroni controlling the FWER at 5%

I reject the genes id that has a p value smaller than 0.05/6033. 

```{r, echo=FALSE}
bonferroni_sig <- which(dat$p_values <= 0.05/6033)
print(paste("Number of significant genes:", length(bonferroni_sig)))
```

We reject: 332, 610, 1720

(b) Holm’s procedure controlling the FWER at 5%

Following the Holm’s procedure, I reject the ids where the sorted p-value is smaller or equal to 0.05/(6033+1-i), where i is the rank number of the sorted p-value. The ids we want to rejected here the same as before, but Holm’s procedure usually rejects more than Bonferroni.

```{r, echo=FALSE}
dat$id <- 1:6033
sorted_data = dat[order(dat$p_values), ]
for (i in 1:6033) {
  if (sorted_data[i, 2] > 0.05/(6033+1-i)){
    cat("We accept when i equals", i, "\n") 
    break
  } 
}

```
We reject: 610, 1720, 332.

(c) Benjamini and Hochberg’s FDR control, apply with control at the 10% level

I will find the smallest index of sorted p-value that bigger than (i/6033)*0.1. Thus, we should reject the first 60 p-value of the sorted p-value since 61 is my smallest index.

```{r, echo=FALSE}
for (i in 1:6033) {
  if (sorted_data[i, 2] > (i/6033)*0.1){
    cat("We accept when i equals", i, " \n")
    break
  }
}
```

We reject : 610 ,1720 ,332 ,364 ,914 ,3940 ,4546 ,1068 ,579 ,4331 ,1089 ,3647 ,1113 ,1557 ,1077 ,4518 ,4088 ,3991 ,3375 ,4316 ,4073 ,1130 ,3665 ,735 ,4549 ,1346 ,921 ,1589 ,1314 ,4981 ,4104 ,2897 ,739 ,702 ,2 ,4000 ,2370 ,3282 ,2856 ,3600 ,2945 ,905 ,694 ,3017 ,4396 ,4552 ,721 ,1588 ,3292 ,3930 ,698 ,3260 ,4154 ,11 ,3505 ,4040 ,377 ,3269 ,805 ,637

(d) Control the expected number of false discoveries (EFD). Find the genes that correspond to EFD of 1, and then also with 5

Below is a truncated scatter plot for observed p-value in (0, 0.05) since we only need to check the p-value smaller than 0.01 and 0.05.  

```{r, echo=FALSE}
par(mfrow = c(1, 2)) 
sorted_data$expect_p <- (1:6033)/6034
plot(x = sorted_data$expect_p, y = sorted_data$p_values, pch = 20, xlab = "Expected p-value", ylab = "Observed p-value", main = "Expect and Observe") 
abline(h = 0.01, col = "blue")
abline(h = 0.05, col = "red")

plot(x = sorted_data$expect_p, y = sorted_data$p_values, pch = 20, xlab = "Expected p-value", ylab = "Observed p-value",main = "Truncated Expect and Observe", ylim = c (0.0, 0.05))
abline(h = 0.01, col = "blue") 
abline(h = 0.05, col = "red")
```

```{r, echo=FALSE}
cat("For EFD of 1, we reject : \n")
sorted_data[which(sorted_data$p_values <= 0.01), ]$id
cat("--------------- total: 172 genes\n")
cat("\n")
cat("For EFD of 5, we reject : \n")
sorted_data[which(sorted_data$p_values <= 0.05), ]$id
cat("--------------- total: 478 genes")
```


(e) Storey’s q-values with a pFDR threshold of 10%. Also give the estimate of the proportion of null genes $\pi_0$.  

Using the package provided above, I get the estimated proportion of null genes $\pi_0 = 0.8545$ and we should reject:

```{r, echo=FALSE}
#install.packages("~/Downloads/qvalue_2.30.0.tgz", repos = NULL, type = .Platform$pkgType)
library(qvalue)
storey_results <- qvalue(dat$p_values)
storey_sig <- which(storey_results$qvalue < 0.1)
print(paste("Number of significant genes: ", length(storey_sig)))
cat("Significant genes: ")
print(storey_sig)
print(paste("Estimated proportion of null genes: ", storey_results$pi0))
```

### 2.3

In this study, I examined the genetic activity data on 52 patients with cancer and 50 controls. Among 6033 genes information, my goals is to find genes that are expressed differently between the groups. The data I use includes z scores which quantify the level of expression of each of the 6033 genes. Using these information, I calculated my p values, which shows us how likely it is that my data would have occurred under my null hypothesis (no differential expression). While performing the multiple hypothesis tests, I used 5 different methods to control the family-wise error rate and the false discovery rate. These methods help reduce false positive results, which may occur due to randomness.

From the result of the analysis, I found a great amount of genes with statistically significant differential expression between the patient group and the control group. The standard for significance I used differ by the probability of making a false positive error. The Bonferroni and the Holm’s procedure control the family-wise error rate and make sure that the probability of making false discovery is very small. But they might also results in a high probability of false negatives, and meanwhile fail to identify true positives. On the other hand, the Benjamini and Hochberg’s FDR control and the Storey’s q-value method allow for a greater chance of false positives, but also do well on identifying the true positives. No matter which method to use, we should always make wise choice based on our scientific goals and evaluate the trade-off between false positives and false negatives.

## Question 3

Suppose we test m = 10 hypotheses

### 3.1

Suppose that we wish to control the Type I error for each null hypothesis at level $\alpha = 0.05$. I use the Bonferroni method. We should reject all the p-value smaller or equal to 0.005 (0.05/10 = 0.005), thus we should reject $H_{01}$, $H_{09}$, $H_{10}$.

```{r, echo=FALSE}
df <- data.frame(p = c(0.0011, 0.031, 0.017, 0.32, 0.11, 0.90, 0.07, 0.006, 0.004, 0.0009),
id = 1:10)
sorted_df <- df[order(df$p), ]
```

### 3.2

Now suppose that we wish to control the FWER at level $\alpha = 0.05$. I use the Bonferroni-Holm correction. First, we sort the p-value in increasing order. Then, we compare the p-value with 0.05/(11 - i), where i is the rank index. For this question, we start to accept the hypothesis when i is 5, and reject $H_{10}$, $H_{01}$, $H_{09}$, $H_{08}$.

```{r, echo=FALSE}
for (i in 1:10) {
if (sorted_df[i, 1] > 0.05/(11 - i)) {
  cat("Accept when i equals", i, " \n")
  break
    } 
  }
cat("Therefore, we reject: ") 
for (j in 1:i-1) {
  print(sorted_df[j, 2]) 
  }
```

### 3.3

Now suppose that we wish to control the FDR at level q = 0.05. I use the Benjamini and Hochberg’s FDR control. I will now compare the p-value with (i/10)*0.05, where i is the rank of our p-value. We start to accept the hypothesis when i = 6, so we reject 5 hypothesis $H_{10}$, $H_{01}$, $H_{09}$, $H_{08}$, $H_{03}$.

```{r, echo=FALSE}
for (i in 1:10) {
if (sorted_df[i, 1] > (i/10)*0.05) {
  cat("Accept when i equals", i, "\n")
  break
    } 
  }
cat("Therefore, we reject :") 
for (j in 1:i-1) {
  print(sorted_df[j, 2]) 
  }
```

### 3.4

Now suppose that we wish to control the FDR at level q = 0.2. Using the same method, I accept when i = 9, so I reject $H_{10}$, $H_{01}$, $H_{09}$, $H_{08}$, $H_{03}$, $H_{02}$, $H_{07}$, $H_{05}$.

```{r, echo=FALSE}
for (i in 1:10) {
if (sorted_df[i, 1] > (i/10)*0.2) {
  cat("Accept when i equals", i, "\n")
  break
    } 
  }
cat("Therefore, we reject :") 
for (j in 1:i-1) {
  print(sorted_df[j, 2]) 
  }
```

### 3.5

With q = 0.2, I reject 8 hypothesis, but from these hypothesis, we only have $H_{10}$, $H_{01}$, $H_{09}$, $H_{08}$, $H_{03}$, $H_{02}$ with a p-value < 0.05. Thus, our approximately false positives of the null hypothesis rejected at q = 0.2 is 6/8 = 0.75.




